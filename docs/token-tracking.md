# Token Tracking & Cache Display

Design, implementation, and Anthropic Prompt Caching behavior of the token usage tracking system.

---

## Architecture Overview

```
Anthropic API Response
    ↓ (usage: input_tokens, cache_creation_input_tokens, cache_read_input_tokens)
LangChain ChatAnthropic (_create_usage_metadata)
    ↓ (usage_metadata dict, input_tokens already includes cache)
Agent stream_events() → TokenTracker.update(chunk)
    ↓ (_extract_usage → merge into _current_turn)
TokenTracker.finalize_turn()
    ↓ (accumulate into _total, return current turn's TokenUsageInfo)
StreamEventEmitter.token_usage()
    ↓ (event dict)
CLI StreamState → format_turn_token_usage() / display_token_usage()
    ↓
Terminal display
```

### Key Components

| File | Responsibility |
|------|----------------|
| `stream/token_tracker.py` | Extract usage from chunks, merge and accumulate, finalize per-turn |
| `stream/emitter.py` | Convert TokenUsageInfo to event dict |
| `agent.py` (`stream_events`) | Orchestrate token tracking flow, handle parallel tool batches |
| `cli.py` | Format event dict for terminal display |

---

## Anthropic Prompt Caching Mechanism

### What is Prompt Caching

Anthropic's prompt caching allows fixed content like the system prompt to be cached server-side. Subsequent API calls can read directly from the cache, avoiding reprocessing the same tokens.

> For a detailed explanation of progressive caching, multi-turn behavior, and Extended Thinking cache invalidation, see [docs/prompt-caching.md](prompt-caching.md).

### How to Enable

Caching is enabled at two levels:

1. **System prompt** — `cache_control` on content blocks in `prompts.py`
2. **Messages (progressive)** — `_get_request_payload` override in both `azure_claude.py` and `agent.py` auto-adds `cache_control` to the last message block on every API call

```python
# prompts.py — system-level breakpoints
blocks = [
    {"type": "text", "text": prompt_text, "cache_control": {"type": "ephemeral"}},
]
if skills:
    blocks.append({"type": "text", "text": skills_text, "cache_control": {"type": "ephemeral"}})

# azure_claude.py / agent.py — message-level breakpoint (progressive)
def _get_request_payload(self, input_, *, stop=None, **kwargs):
    kwargs.setdefault("cache_control", {"type": "ephemeral"})
    return super()._get_request_payload(input_, stop=stop, **kwargs)
```

`"type": "ephemeral"` indicates a cache with a 5-minute TTL.

### Token Fields Returned by the API

The Anthropic API `usage` object contains:

| Field | Description |
|-------|-------------|
| `input_tokens` | **Non-cached** input tokens (does not include cache portion) |
| `cache_creation_input_tokens` | Tokens **written to cache** in this call (first creation) |
| `cache_read_input_tokens` | Tokens **read from cache** in this call |
| `output_tokens` | Output tokens generated by the model |

**Important**: Anthropic's raw `input_tokens` **does not include** cache tokens.

### LangChain's Transformation

LangChain merges all three in `_create_usage_metadata`:

```python
# langchain_anthropic/chat_models.py
input_tokens = (
    (getattr(anthropic_usage, "input_tokens", 0) or 0)        # non-cached input
    + (input_token_details["cache_read"] or 0)                 # cache read
    + (input_token_details["cache_creation"] or 0)             # cache write
)
```

So **LangChain's `input_tokens` already includes cache tokens**. The final `usage_metadata` structure is:

```python
{
    "input_tokens": 3625,       # = 356 (new) + 3269 (cache)
    "output_tokens": 155,
    "total_tokens": 3780,
    "input_token_details": {
        "cache_creation": 3269,  # tokens written to cache (first time)
        "cache_read": 0,         # tokens read from cache
    }
}
```

### Cache Lifecycle

```
First API call (cold cache):
    input_tokens = 356 (new) + 3269 (cache_creation) = 3625
    → system prompt is written to cache

Subsequent calls (within 5 minutes, warm cache):
    input_tokens = 1431 (new) + 3269 (cache_read) = 4700
    → system prompt is read from cache

After 5 minutes (cache expired):
    input_tokens = 356 (new) + 3269 (cache_creation) = 3625
    → cache is recreated
```

### Billing Impact

| Token Type | Price Multiplier |
|-----------|-----------------|
| Regular input | 1x |
| cache_creation (write) | 1.25x (25% more expensive) |
| cache_read (read) | 0.1x (90% cheaper) |

System prompt cost comparison for a 5-turn session:

- **Without caching**: 3,269 × 5 = 16,345 tokens @ 1x
- **With caching**: 3,269 × 1.25 + 3,269 × 0.1 × 4 = 4,086 + 1,308 = 5,394 tokens equivalent
- **~67% savings**

---

## Token Tracking Implementation

### TokenUsageInfo

```python
@dataclass
class TokenUsageInfo:
    input_tokens: int = 0                    # LangChain's merged input (includes cache)
    output_tokens: int = 0
    total_tokens: int = 0                    # input + output
    cache_creation_input_tokens: int = 0     # tokens written to cache
    cache_read_input_tokens: int = 0         # tokens read from cache
```

Supports the `+` operator for cross-turn accumulation.

### TokenTracker.update() — Merge Strategy

Extracts token stats from `AIMessageChunk.usage_metadata`. Uses **merge (take max)** instead of replace:

```python
# Take max to preserve non-zero values from each chunk
cur = self._current_turn
self._current_turn = TokenUsageInfo(
    input_tokens=max(cur.input_tokens, input_tokens),
    output_tokens=max(cur.output_tokens, output_tokens),
    total_tokens=...,
    cache_creation_input_tokens=max(cur.cache_creation_input_tokens, cache_creation),
    cache_read_input_tokens=max(cur.cache_read_input_tokens, cache_read),
)
```

**Why merge instead of replace?**

During streaming, usage may be spread across multiple chunks:

| Chunk | Contains |
|-------|----------|
| message_start | input_tokens + cache info |
| message_delta | output_tokens |

If using replace, the second chunk would overwrite the first chunk's cache data (since it only has output_tokens, with cache fields at 0).

Using `max()` merge ensures data from both chunks is preserved.

> **Note**: Current LangChain (v1.3.1) actually sends complete aggregated usage in `message_delta`, so only one usage chunk is received per turn. However, the merge strategy is more robust and can handle behavior changes in future LangChain versions.

### Parallel Tool Handling

A single API call may return multiple tool_calls (executed in parallel). Token usage is only sent after the last tool_result of that batch:

```python
# agent.py stream_events()
if turn_usage and not turn_usage.is_empty():
    # First tool result (has usage data)
    pending_turn_usage = turn_usage
    parallel_count = 1
elif pending_turn_usage is not None:
    # Subsequent tool in the same batch (finalize returns None)
    parallel_count += 1
```

Flow:

```
API Call → tool_call_1, tool_call_2
         → tool_result_1: finalize_turn() returns usage, parallel_count=1
         → tool_result_2: finalize_turn() returns None, parallel_count=2
New AI chunk → _emit_pending() sends usage (parallel_count=2)
```

---

## Terminal Display Format

### Per-turn Display

Format: `↳ {new} + {cached} cache {type} / {output} out`

```
↳ 356 + 3,269 cache write / 162 out          # First call, cache write
↳ 1,437 + 3,269 cache read / 63 out          # Subsequent call, cache read
↳ 1,583 + 3,269 cache read / 133 out (2 tools)  # Parallel tools
```

Where:
- `356` = new input tokens (non-cached portion) = `input_tokens - cache_read - cache_creation`
- `3,269` = cached tokens (system prompt size)
- `cache write` / `cache read` = cache operation type
- `(2 tools)` = number of tools executed in parallel in that API call

### Total Display

Format depends on cache status:

```
# Cache read only (warm cache):
Tokens: 8,525 + 16,345 cache read = 24,870 in / 692 out

# Both read and write (cold start):
Tokens: 8,497 + 16,345 cache (13,076 read, 3,269 write) = 24,842 in / 679 out

# No caching:
Tokens: 5,000 in / 200 out
```

### How to Interpret

Using this output as an example:

```
Tokens: 8,537 + 16,345 cache (13,076 read, 3,269 write) = 24,882 in / 727 out
```

| Part | Value | Meaning |
|------|-------|---------|
| `8,537` | new input | Conversation content, tool results, and other non-cached tokens |
| `16,345` | cached | System prompt cache tokens (across 5 calls) |
| `13,076 read` | cache read | 4 calls × 3,269 = 13,076 (read from cache) |
| `3,269 write` | cache write | 1 call × 3,269 (first write to cache) |
| `24,882 in` | total input | 8,537 + 16,345 = 24,882 |
| `727 out` | output | All output tokens generated by the model |

Expanded per turn:

| Turn | Tools | New | Cache | Type | Output |
|------|-------|-----|-------|------|--------|
| 1 | load_skill | 356 | 3,269 | write | 162 |
| 2 | list | 1,437 | 3,269 | read | 63 |
| 3 | get ×2 | 1,583 | 3,269 | read | 133 |
| 4 | test ×2 | 2,437 | 3,269 | read | 156 |
| 5 | final reply | 2,724 | 3,269 | read | 213 |
| **Total** | | **8,537** | **16,345** | | **727** |

---

## FAQ

### Q: Why does every run show "cache write"?

Anthropic's prompt cache TTL is **5 minutes**. If more than 5 minutes pass between runs, the cache expires and the first API call needs to recreate the cache.

Running again immediately will show all `cache read` (no write).

### Q: Is cache write wasteful?

No. Cache write costs 25% more than regular input, but subsequent cache reads are 90% cheaper. As long as there are 2+ calls within 5 minutes (a single session typically has 3-6 API calls), caching is already cost-effective.

### Q: Why does new input grow with each turn?

Because each API call sends the full conversation history (previous messages + tool calls + tool results). As the conversation progresses, the history gets longer, so new input increases from 356 → 1,437 → 1,583 → 2,437.

### Q: What is the relationship between input_tokens and cache tokens?

Cache tokens are a **subset** of input_tokens, not additional:

```
input_tokens (LangChain) = new_input + cache_read + cache_creation
         3,625           =    356    +   3,269   +      0
```

### Q: Why doesn't the per-turn display show token usage for the final reply?

The final reply (Turn 5) has no tool_result to trigger the display. Its usage is included in the total summary but is not shown as a separate `↳` line.

---

## Library Versions

- `langchain-anthropic`: 1.3.1
- `langchain-core`: 1.2.7
- `anthropic`: 0.76.0

---

*Document created: 2025-01*
